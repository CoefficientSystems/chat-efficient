{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368dccec-1d58-4167-a50d-01c56d98c24e",
   "metadata": {},
   "source": [
    "<img src=\"../images/coefficient-aidl.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52fb18-e4c3-4d38-86d6-cf10abeca5e8",
   "metadata": {},
   "source": [
    "# Build Your Own Private ChatGPT Super-Assistant Using Streamlit, LangChain, Chroma & Llama 2\n",
    "## Chroma Demo\n",
    "**Questions?** contact@coefficient.ai / [@CoefficientData](https://twitter.com/CoefficientData)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9dfd7f-dcc6-418c-bc6b-02c7771e07fa",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73809ac-30df-4182-8d69-e49d9ccc60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils import scrape_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ecd369-baf5-4c19-9245-1cad96fbd17a",
   "metadata": {},
   "source": [
    "## 1. Chroma Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae2d37-e3fe-4fab-89d4-16cdda33b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Chroma client\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62019417-87d1-4187-b3a9-2ee3d91aa01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccbc06d-74ed-4657-9468-a4ae0d3be0d8",
   "metadata": {},
   "source": [
    "Collections are where you'll store your embeddings, documents, and any additional metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063c858-e0bf-4933-b0d0-29e178c18bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some text documents to the collection\n",
    "collection.add(\n",
    "    documents=[\"This is a document\", \"This is another document\"],\n",
    "    metadatas=[{\"source\": \"my_source\"}, {\"source\": \"my_source\"}],\n",
    "    ids=[\"id1\", \"id2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e82e2a-f63b-4594-9ce5-25c7908ce1fd",
   "metadata": {},
   "source": [
    "Chroma will store your text, and handle tokenization, embedding, and indexing automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b8ae0-c0de-4450-bb41-14e6f2ab0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection2 = chroma_client.create_collection(name=\"another_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d0636-443d-4309-b47a-25d009533044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in pre-generated embeddings\n",
    "collection2.add(\n",
    "    embeddings=[[1.2, 2.3, 4.5], [6.7, 8.2, 9.2]],\n",
    "    documents=[\"This is a document\", \"This is another document\"],\n",
    "    metadatas=[{\"source\": \"my_source\"}, {\"source\": \"my_source\"}],\n",
    "    ids=[\"id1\", \"id2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a57272-0353-4332-9ee5-2109a33a3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "results = collection.query(query_texts=[\"This is a query document\"], n_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e240a34-329a-47b3-8581-b359d35489c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15485f-7519-46ad-bb15-5d2f351b22b8",
   "metadata": {},
   "source": [
    "- **Where is data stored?** By default data stored in Chroma is ephemeral making it easy to prototype scripts.\n",
    "- **Can data be persisted?** It's easy to make Chroma persistent so you can reuse every collection you create and add more documents to it later. It will load your data automatically when you start the client, and save it automatically when you close it.\n",
    "\n",
    "Check out the [Usage Guide](https://docs.trychroma.com/usage-guide) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24f61d-1db6-4021-a851-866b5dd013be",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_client = chromadb.PersistentClient(path=\".\")\n",
    "persistent_collection = persistent_client.create_collection(name=\"example_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12c1ac-bd27-442d-8d1a-5439d5a30abc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5edd4-ddb4-4f43-9488-f2028670190c",
   "metadata": {},
   "source": [
    "## 2. Create embeddings with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d53907-b103-4c48-bad2-88b89de95134",
   "metadata": {},
   "source": [
    "### Create embeddings with Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4790923-7eb0-4f37-a975-a2d6d353b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.llamacpp import LlamaCppEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049bedfa-b52a-4c00-af91-4d861708b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model path is correct!\n",
    "llama_embedder = LlamaCppEmbeddings(model_path=\"../models/llama-2-7b-chat.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5eada-90c6-4972-9ea0-0ea98d0027f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = llama_embedder.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216db12-4768-4765-9d88-5f838a374abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f79cd-dbe5-411d-ab12-d0b6d8395f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202e270-f760-480f-bb1f-301eac40a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_result = llama_embedder.embed_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83562b-c29c-41da-91c9-daa43bb071cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb9ae9-5540-4579-94ef-f390a2bbc4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_result[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bbb194-b084-4949-b9c8-f965e28a5403",
   "metadata": {},
   "source": [
    "### Create embeddings using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac59e22-a591-4a97-a69a-a19c34a902a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some more interesting data\n",
    "url = \"https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper/frontier-ai-capabilities-and-risks-discussion-paper\"\n",
    "paper = scrape_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bfc5ee-cdee-4aa4-ba71-8e5450ca39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek\n",
    "print(f\"{len(paper)=}\\n\\nExtract:\")\n",
    "print(paper[10000:15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7553911-ab93-408f-afbf-0f4a17e14f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to disk - we only do 5000 characters as Llama is very slow at embedding\n",
    "with open(\"frontier-ai-paper.txt\", \"w\") as f:\n",
    "    f.write(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dc727-51ca-4f67-974b-8b8209f9b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the document\n",
    "raw_documents = TextLoader(\"frontier-ai-paper.txt\").load()\n",
    "\n",
    "# Split it into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94779f67-1c94-4a81-875f-bfa8d016108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2d338-61ba-4990-8f10-f40bd2c8fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493fd741-9062-4063-97dd-2365e0f2d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Embed each chunk and load it into the vector store\n",
    "# db = Chroma.from_documents(documents, llama_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc87d1e4-d83e-4144-86e9-7df7b6cdd498",
   "metadata": {},
   "source": [
    "### Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d7eb3-83cc-4317-925f-fbd791cdfc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are scaffolds in AI?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a787276-c0f2-4449-898b-d8c226f4b25e",
   "metadata": {},
   "source": [
    "## Using SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878b1b6-1253-458d-b5c6-1d6f2e5a4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the new embedder\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "st_embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a8b09c-40c2-4734-901b-eefeb39ef43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compare this with SentenceTransformerEmbeddings\n",
    "db2 = Chroma.from_documents(documents, st_embedder, collection_name=\"st_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7581e9a-34f5-4e16-a10a-3ba8d96c971c",
   "metadata": {},
   "source": [
    "**Note: It takes `SentenceTransformerEmbeddings` <1 second, and Llama 2 several minutes!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503e648-e7a2-4f36-8545-08c4c9776da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the whole paper this time, Sentence-Transformers can handle it\n",
    "print(f\"{len(paper)=}\")\n",
    "with open(\"frontier-ai-paper.txt\", \"w\") as f:\n",
    "    f.write(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2684eca-aeb0-43e5-b65f-b81b53fca512",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = TextLoader(\"frontier-ai-paper.txt\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910e392-b4fd-40a4-a602-10957c701469",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4052d-f962-4580-bf3a-49d3ac224756",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "db2 = Chroma.from_documents(documents, st_embedder, collection_name=\"st_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c9f18e-9f93-42d0-9abc-dcfa40d74cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db2.similarity_search(\"What are scaffolds in AI?\")\n",
    "print(docs[0].page_content.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b514178-3b7c-4f56-b96f-5489f87c856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db2.similarity_search(\"What are the top risks of frontier models?\")\n",
    "print(docs[0].page_content.replace(\"\\n\", \" \"), \"\\n\\n\")\n",
    "print(docs[1].page_content.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44934f7-6077-4e87-a172-5367d6163f53",
   "metadata": {},
   "source": [
    "### Maximum marginal relevance search (MMR)\n",
    "Maximal marginal relevance optimizes for similarity to query and diversity among selected documents. It is also supported in async API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a828a-21ef-4e64-a50a-38e7c85262b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the top risks of frontier models?\"\n",
    "retriever = db2.as_retriever(search_type=\"mmr\")\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(docs[0].page_content.replace(\"\\n\", \" \"), \"\\n\\n\")\n",
    "print(docs[1].page_content.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84226ab-965a-4001-b4de-5a1215ea8632",
   "metadata": {},
   "source": [
    "### Deep linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324306fd-c201-4fe4-b33c-d41dd94d2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db2.similarity_search(\"Which model has the best benchmark?\")\n",
    "result = docs[1].page_content\n",
    "print(result.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75935907-78c2-478b-834c-2f0ad7a9c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50e576-79cd-4a7b-968b-73873d55ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_result = urllib.parse.quote(result[:50])\n",
    "encoded_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11de357-9267-4b34-94cc-0f40d527fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplink = f\"{url}#:~:text={encoded_result}\"\n",
    "deeplink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8646efa-78bd-4ef8-baee-31aff6165fa7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2a2e3-c1a4-41ab-9541-3f04d8cd4bb7",
   "metadata": {},
   "source": [
    "## 3. Exercise: Q&A bot with vector database\n",
    "\n",
    "> Combine the Chroma vector database with a Llama-based LangChain LLM to create a Q&A bot for the provided (or any other) URL.\n",
    "> Tips:\n",
    "> - Encode your queries using the Sentence-Transformer embedding & return the top documents\n",
    "> - Include the question alongside the top N documents into your LangChain LLM's context window\n",
    "> - Use Llama 2 to synthesise a coherent answer\n",
    ">\n",
    "> This approach enables LLMs to answer questions to things they haven't been pre-trained on by using the vector database as an \"encyclopedia\" that it can reference as needed. This is known as \"retrieval-augmented generation\" or \"RAG\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e74d5-fde3-4e0a-b4bd-e61b4d41079a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bdb3b81-ca58-43fc-9d50-a68395b79e37",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42846d93-782b-4f49-9080-a5149d26fac5",
   "metadata": {},
   "source": [
    "## Where next?\n",
    "\n",
    "LangChain is far more powerful than we've seen so far! Here's an idea of what else you can do:\n",
    "- [Learn to use agents and tools with LangChain](https://python.langchain.com/docs/modules/agents/tools/) such as searching the web, querying APIs, reading papers on ArXiv, checking the weather, digesting articles on Wikipedia, making (and transcribing) calls with Twilio, accessing financial data and much more. Check out the [list of integrations here](https://python.langchain.com/docs/integrations/tools).\n",
    "- [Query a SQL database](https://python.langchain.com/docs/expression_language/cookbook/sql_db) with LangChain Runnables\n",
    "- [Write Python code](https://python.langchain.com/docs/expression_language/cookbook/code_writing) with LangChain\n",
    "- [Learn more about RAG](https://python.langchain.com/docs/expression_language/cookbook/retrieval) or use [this example to combine agents with the Chroma vector store](https://python.langchain.com/docs/modules/agents/how_to/agent_vectorstore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatefficient)",
   "language": "python",
   "name": "chatefficient"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
